
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/02_deeplearning/03_example_2step_early_stopping.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_02_deeplearning_03_example_2step_early_stopping.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_02_deeplearning_03_example_2step_early_stopping.py:


Example 03: 2-step Early Stopping
=================================

This example demonstrates a two-step training strategy using early stopping
with ``rosoku`` for deep-learning-based EEG classification.

The purpose of this example is to illustrate a practical way to estimate a
reasonable loss target and then use it to control the final training process,
while keeping the training loop unchanged.

The training is split into two stages:

1. **Step 1 (loss calibration / warm-up)**:
   A validation set is temporarily introduced and standard early stopping is
   applied.
   The goal of this step is to obtain a **rough estimate of an achievable loss
   value** and a stable training regime.
   The best loss value and corresponding checkpoint are stored.

2. **Step 2 (final training)**:
   The model is retrained using a larger training set that also includes the
   former validation data.
   Early stopping is now controlled by a custom ``callback_early_stopping``
   function, which stops training once the training loss reaches the best loss
   observed in Step 1.

This strategy is useful when:

- Early stopping is needed, but a permanent validation split is undesirable
- One wants to **maximize the amount of data used for final training**
- A loss-based stopping criterion should be **derived empirically**
  rather than hand-tuned

Key aspects illustrated in this example include:

- Using validation loss only for **loss calibration**
- Custom early stopping via ``callback_early_stopping``
- Reusing a loss threshold stored in a checkpoint
- Implementing advanced stopping logic without modifying the training loop

This example focuses on the *training strategy* rather than absolute performance,
and serves as a practical template for flexible early-stopping workflows in
``rosoku``.

.. GENERATED FROM PYTHON SOURCE LINES 46-51

.. code-block:: Python


    # Authors: Simon Kojima <simon.kojima@inria.fr>
    #
    # License: BSD (3-clause)








.. GENERATED FROM PYTHON SOURCE LINES 52-63

Set Environment Variables for Replicability
===========================================
NOTE:
This environment variable MUST be set **before importing torch**.
It enforces deterministic behavior in CUDA CuBLAS operations
when `torch.use_deterministic_algorithms(True)` is enabled.

See:
https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility

If this variable is set after importing torch, it will have no effect.

.. GENERATED FROM PYTHON SOURCE LINES 63-67

.. code-block:: Python

    import os

    os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"








.. GENERATED FROM PYTHON SOURCE LINES 68-70

Import Packages
===============

.. GENERATED FROM PYTHON SOURCE LINES 70-80

.. code-block:: Python

    import functools
    from pathlib import Path
    import mne
    import torch
    import braindecode
    import rosoku

    from moabb.datasets import Dreyer2023









.. GENERATED FROM PYTHON SOURCE LINES 81-83

Define callback functions
=========================

.. GENERATED FROM PYTHON SOURCE LINES 83-154

.. code-block:: Python

    def callback_get_model(X, y):
        _, n_chans, n_times = X.shape
        F1 = 4
        D = 2
        F2 = F1 * D

        model = braindecode.models.EEGNet(
            n_chans=n_chans,
            n_outputs=2,
            n_times=n_times,
            F1=F1,
            D=D,
            F2=F2,
            drop_prob=0.5,
        )

        return model


    def callback_load_epochs(
        items, split, dataset, l_freq, h_freq, order_filter, tmin, tmax
    ):
        subject = items[0]
        items = items[1:]

        sessions = dataset.get_data(subjects=[subject])
        raws_dict = sessions[subject]["0"]

        epochs_list = []

        for name_run, raw in raws_dict.items():
            if not True in [item in name_run for item in items]:
                continue

            raw.filter(
                l_freq=l_freq,
                h_freq=h_freq,
                method="iir",
                iir_params={"ftype": "butter", "order": order_filter, "btype": "bandpass"},
            )

            raw = raw.pick(picks="eeg")

            epochs = mne.Epochs(
                raw=raw,
                tmin=tmin,
                tmax=tmax,
                baseline=None,
            )

            epochs_list.append(epochs)

        return mne.concatenate_epochs(epochs_list)


    def callback_proc_epochs(epochs, split):
        # do nothing in this example
        return epochs


    def convert_epochs_to_ndarray(
        epochs,
        split,
        label_keys,
    ):
        X = epochs.get_data()
        y = rosoku.utils.get_labels_from_epochs(epochs, label_keys)

        return X, y









.. GENERATED FROM PYTHON SOURCE LINES 155-157

Run the Experiment using Early Stopping with Validation data
============================================================

.. GENERATED FROM PYTHON SOURCE LINES 157-227

.. code-block:: Python


    subject = 10
    resample = 128

    lr = 5e-4
    weight_decay = 1e-2
    n_epochs = 500
    batch_size = 8
    patience = 75
    device = "cuda" if torch.cuda.is_available() else "cpu"

    seed = 42

    dataset = Dreyer2023()

    save_base = Path("~").expanduser() / "rosoku-log"
    (save_base / "checkpoint").mkdir(parents=True, exist_ok=True)
    (save_base / "history").mkdir(parents=True, exist_ok=True)
    (save_base / "saliency").mkdir(parents=True, exist_ok=True)
    (save_base / "samples").mkdir(parents=True, exist_ok=True)
    (save_base / "normalization").mkdir(parents=True, exist_ok=True)

    criterion = torch.nn.CrossEntropyLoss()
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR
    scheduler_params = {"T_max": n_epochs, "eta_min": 1e-6}
    optimizer = torch.optim.AdamW
    optimizer_params = {"lr": lr, "weight_decay": weight_decay}
    early_stopping = rosoku.utils.EarlyStopping(patience=patience)

    label_keys = {"left_hand": 0, "right_hand": 1}

    results_1st_step = rosoku.deeplearning(
        items_train=[subject, "R1", "R2", "R3"],
        items_valid=[subject, "R4"],
        items_test=[[subject, "R5", "R6"]],
        callback_load_epochs=functools.partial(
            callback_load_epochs,
            dataset=dataset,
            l_freq=8.0,
            h_freq=30.0,
            order_filter=4,
            tmin=dataset.interval[0] + 0.5,
            tmax=dataset.interval[1],
        ),
        callback_proc_epochs=callback_proc_epochs,
        callback_convert_epochs_to_ndarray=functools.partial(
            convert_epochs_to_ndarray, label_keys=label_keys
        ),
        batch_size=batch_size,
        n_epochs=n_epochs,
        criterion=criterion,
        optimizer=optimizer,
        optimizer_params=optimizer_params,
        callback_get_model=callback_get_model,
        scheduler=scheduler,
        scheduler_params=scheduler_params,
        device=device,
        early_stopping=early_stopping,
        history_fname=(save_base / "history" / f"sub-{subject}.parquet"),
        checkpoint_fname=(save_base / "checkpoint" / f"sub-{subject}.pth"),
        samples_fname=(save_base / "samples" / f"sub-{subject}.parquet"),
        normalization_fname=(save_base / "normalization" / f"sub-{subject}.msgpack"),
        saliency_map_fname=(save_base / "saliency" / f"sub-{subject}.msgpack"),
        label_keys=label_keys,
        seed=seed,
        additional_values={"subject": subject},
        use_deterministic_algorithms=True,
        min_delta=0,
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    0it [00:00, ?it/s]    9it [00:00, 41210.41it/s]
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Filtering raw data in 1 contiguous segment
    Setting up band-pass filter from 8 - 30 Hz

    IIR filter parameters
    ---------------------
    Butterworth bandpass zero-phase (two-pass forward and reverse) non-causal filter:
    - Filter order 16 (effective, after forward-backward)
    - Cutoffs at 8.00, 30.00 Hz: -6.02, -6.02 dB

    Used Annotations descriptions: [np.str_('left_hand'), np.str_('right_hand')]
    Ignoring annotation durations and creating fixed-duration epochs around annotation onsets.
    Not setting metadata
    40 matching events found
    No baseline correction applied
    0 projection items activated
    Filtering raw data in 1 contiguous segment
    Setting up band-pass filter from 8 - 30 Hz

    IIR filter parameters
    ---------------------
    Butterworth bandpass zero-phase (two-pass forward and reverse) non-causal filter:
    - Filter order 16 (effective, after forward-backward)
    - Cutoffs at 8.00, 30.00 Hz: -6.02, -6.02 dB

    Used Annotations descriptions: [np.str_('left_hand'), np.str_('right_hand')]
    Ignoring annotation durations and creating fixed-duration epochs around annotation onsets.
    Not setting metadata
    40 matching events found
    No baseline correction applied
    0 projection items activated
    Filtering raw data in 1 contiguous segment
    Setting up band-pass filter from 8 - 30 Hz

    IIR filter parameters
    ---------------------
    Butterworth bandpass zero-phase (two-pass forward and reverse) non-causal filter:
    - Filter order 16 (effective, after forward-backward)
    - Cutoffs at 8.00, 30.00 Hz: -6.02, -6.02 dB

    Used Annotations descriptions: [np.str_('left_hand'), np.str_('right_hand')]
    Ignoring annotation durations and creating fixed-duration epochs around annotation onsets.
    Not setting metadata
    40 matching events found
    No baseline correction applied
    0 projection items activated
    /home/skojima/git/rosoku/examples/02_deeplearning/03_example_2step_early_stopping.py:135: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.
      return mne.concatenate_epochs(epochs_list)
    Using data from preloaded Raw for 40 events and 2305 original time points ...
    0 bad epochs dropped
    Using data from preloaded Raw for 40 events and 2305 original time points ...
    0 bad epochs dropped
    Using data from preloaded Raw for 40 events and 2305 original time points ...
    0 bad epochs dropped
    Using data from preloaded Raw for 40 events and 2305 original time points ...
    Using data from preloaded Raw for 40 events and 2305 original time points ...
    Using data from preloaded Raw for 40 events and 2305 original time points ...
    Not setting metadata
    120 matching events found
    No baseline correction applied
    0it [00:00, ?it/s]    9it [00:00, 42177.36it/s]
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Filtering raw data in 1 contiguous segment
    Setting up band-pass filter from 8 - 30 Hz

    IIR filter parameters
    ---------------------
    Butterworth bandpass zero-phase (two-pass forward and reverse) non-causal filter:
    - Filter order 16 (effective, after forward-backward)
    - Cutoffs at 8.00, 30.00 Hz: -6.02, -6.02 dB

    Used Annotations descriptions: [np.str_('left_hand'), np.str_('right_hand')]
    Ignoring annotation durations and creating fixed-duration epochs around annotation onsets.
    Not setting metadata
    40 matching events found
    No baseline correction applied
    0 projection items activated
    /home/skojima/git/rosoku/examples/02_deeplearning/03_example_2step_early_stopping.py:135: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.
      return mne.concatenate_epochs(epochs_list)
    Using data from preloaded Raw for 40 events and 2305 original time points ...
    0 bad epochs dropped
    Using data from preloaded Raw for 40 events and 2305 original time points ...
    Not setting metadata
    40 matching events found
    No baseline correction applied
    0it [00:00, ?it/s]    9it [00:00, 43389.35it/s]
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Filtering raw data in 1 contiguous segment
    Setting up band-pass filter from 8 - 30 Hz

    IIR filter parameters
    ---------------------
    Butterworth bandpass zero-phase (two-pass forward and reverse) non-causal filter:
    - Filter order 16 (effective, after forward-backward)
    - Cutoffs at 8.00, 30.00 Hz: -6.02, -6.02 dB

    Used Annotations descriptions: [np.str_('left_hand'), np.str_('right_hand')]
    Ignoring annotation durations and creating fixed-duration epochs around annotation onsets.
    Not setting metadata
    40 matching events found
    No baseline correction applied
    0 projection items activated
    Filtering raw data in 1 contiguous segment
    Setting up band-pass filter from 8 - 30 Hz

    IIR filter parameters
    ---------------------
    Butterworth bandpass zero-phase (two-pass forward and reverse) non-causal filter:
    - Filter order 16 (effective, after forward-backward)
    - Cutoffs at 8.00, 30.00 Hz: -6.02, -6.02 dB

    Used Annotations descriptions: [np.str_('left_hand'), np.str_('right_hand')]
    Ignoring annotation durations and creating fixed-duration epochs around annotation onsets.
    Not setting metadata
    40 matching events found
    No baseline correction applied
    0 projection items activated
    /home/skojima/git/rosoku/examples/02_deeplearning/03_example_2step_early_stopping.py:135: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.
      return mne.concatenate_epochs(epochs_list)
    Using data from preloaded Raw for 40 events and 2305 original time points ...
    0 bad epochs dropped
    Using data from preloaded Raw for 40 events and 2305 original time points ...
    0 bad epochs dropped
    Using data from preloaded Raw for 40 events and 2305 original time points ...
    Using data from preloaded Raw for 40 events and 2305 original time points ...
    Not setting metadata
    80 matching events found
    No baseline correction applied
    epoch 000, train_loss: 0.6935, train_acc: 0.52, valid_loss: 0.6919, valid_acc: 0.55, lr: 5.0000e-04, et: 0.0665, checkpoint saved
    epoch 001, train_loss: 0.7118, train_acc: 0.51, valid_loss: 0.6909, valid_acc: 0.55, lr: 4.9998e-04, et: 0.0656, checkpoint saved
    epoch 002, train_loss: 0.6905, train_acc: 0.53, valid_loss: 0.6903, valid_acc: 0.53, lr: 4.9996e-04, et: 0.0639, checkpoint saved
    epoch 003, train_loss: 0.7005, train_acc: 0.50, valid_loss: 0.6885, valid_acc: 0.57, lr: 4.9992e-04, et: 0.0599, checkpoint saved
    epoch 004, train_loss: 0.6518, train_acc: 0.64, valid_loss: 0.6875, valid_acc: 0.60, lr: 4.9988e-04, et: 0.0601, checkpoint saved
    epoch 005, train_loss: 0.6430, train_acc: 0.64, valid_loss: 0.6870, valid_acc: 0.55, lr: 4.9982e-04, et: 0.0619, checkpoint saved
    epoch 006, train_loss: 0.6361, train_acc: 0.65, valid_loss: 0.6852, valid_acc: 0.53, lr: 4.9976e-04, et: 0.0611, checkpoint saved
    epoch 007, train_loss: 0.6441, train_acc: 0.62, valid_loss: 0.6849, valid_acc: 0.50, lr: 4.9968e-04, et: 0.0614, checkpoint saved
    epoch 008, train_loss: 0.6243, train_acc: 0.69, valid_loss: 0.6821, valid_acc: 0.55, lr: 4.9960e-04, et: 0.0622, checkpoint saved
    epoch 009, train_loss: 0.6226, train_acc: 0.67, valid_loss: 0.6805, valid_acc: 0.53, lr: 4.9951e-04, et: 0.0629, checkpoint saved
    epoch 010, train_loss: 0.5925, train_acc: 0.73, valid_loss: 0.6803, valid_acc: 0.53, lr: 4.9940e-04, et: 0.0675, checkpoint saved
    epoch 011, train_loss: 0.5997, train_acc: 0.70, valid_loss: 0.6773, valid_acc: 0.57, lr: 4.9929e-04, et: 0.0640, checkpoint saved
    epoch 012, train_loss: 0.6108, train_acc: 0.72, valid_loss: 0.6761, valid_acc: 0.57, lr: 4.9917e-04, et: 0.0640, checkpoint saved
    epoch 013, train_loss: 0.5780, train_acc: 0.75, valid_loss: 0.6744, valid_acc: 0.55, lr: 4.9904e-04, et: 0.0627, checkpoint saved
    epoch 014, train_loss: 0.5828, train_acc: 0.71, valid_loss: 0.6730, valid_acc: 0.55, lr: 4.9889e-04, et: 0.0637, checkpoint saved
    epoch 015, train_loss: 0.5600, train_acc: 0.78, valid_loss: 0.6732, valid_acc: 0.55, lr: 4.9874e-04, et: 0.0642
    epoch 016, train_loss: 0.5654, train_acc: 0.76, valid_loss: 0.6732, valid_acc: 0.53, lr: 4.9858e-04, et: 0.0648
    epoch 017, train_loss: 0.5820, train_acc: 0.70, valid_loss: 0.6721, valid_acc: 0.53, lr: 4.9841e-04, et: 0.0630, checkpoint saved
    epoch 018, train_loss: 0.5466, train_acc: 0.76, valid_loss: 0.6740, valid_acc: 0.53, lr: 4.9822e-04, et: 0.0632
    epoch 019, train_loss: 0.5575, train_acc: 0.78, valid_loss: 0.6723, valid_acc: 0.50, lr: 4.9803e-04, et: 0.0625
    epoch 020, train_loss: 0.5163, train_acc: 0.82, valid_loss: 0.6687, valid_acc: 0.53, lr: 4.9783e-04, et: 0.0638, checkpoint saved
    epoch 021, train_loss: 0.5281, train_acc: 0.75, valid_loss: 0.6713, valid_acc: 0.50, lr: 4.9762e-04, et: 0.0623
    epoch 022, train_loss: 0.5265, train_acc: 0.78, valid_loss: 0.6707, valid_acc: 0.53, lr: 4.9740e-04, et: 0.0623
    epoch 023, train_loss: 0.5258, train_acc: 0.82, valid_loss: 0.6685, valid_acc: 0.55, lr: 4.9717e-04, et: 0.0630, checkpoint saved
    epoch 024, train_loss: 0.4992, train_acc: 0.84, valid_loss: 0.6681, valid_acc: 0.55, lr: 4.9693e-04, et: 0.0629, checkpoint saved
    epoch 025, train_loss: 0.4989, train_acc: 0.82, valid_loss: 0.6713, valid_acc: 0.55, lr: 4.9668e-04, et: 0.0627
    epoch 026, train_loss: 0.4908, train_acc: 0.78, valid_loss: 0.6700, valid_acc: 0.53, lr: 4.9642e-04, et: 0.0637
    epoch 027, train_loss: 0.4803, train_acc: 0.78, valid_loss: 0.6681, valid_acc: 0.53, lr: 4.9615e-04, et: 0.0635, checkpoint saved
    epoch 028, train_loss: 0.4684, train_acc: 0.80, valid_loss: 0.6698, valid_acc: 0.55, lr: 4.9587e-04, et: 0.0626
    epoch 029, train_loss: 0.4993, train_acc: 0.76, valid_loss: 0.6751, valid_acc: 0.53, lr: 4.9558e-04, et: 0.0630
    epoch 030, train_loss: 0.4648, train_acc: 0.87, valid_loss: 0.6764, valid_acc: 0.53, lr: 4.9528e-04, et: 0.0625
    epoch 031, train_loss: 0.4433, train_acc: 0.85, valid_loss: 0.6741, valid_acc: 0.53, lr: 4.9497e-04, et: 0.0616
    epoch 032, train_loss: 0.4930, train_acc: 0.81, valid_loss: 0.6707, valid_acc: 0.53, lr: 4.9466e-04, et: 0.0636
    epoch 033, train_loss: 0.4459, train_acc: 0.84, valid_loss: 0.6686, valid_acc: 0.53, lr: 4.9433e-04, et: 0.0639
    epoch 034, train_loss: 0.4675, train_acc: 0.83, valid_loss: 0.6673, valid_acc: 0.53, lr: 4.9399e-04, et: 0.0610, checkpoint saved
    epoch 035, train_loss: 0.4132, train_acc: 0.86, valid_loss: 0.6784, valid_acc: 0.53, lr: 4.9364e-04, et: 0.0604
    epoch 036, train_loss: 0.4611, train_acc: 0.82, valid_loss: 0.6818, valid_acc: 0.55, lr: 4.9329e-04, et: 0.0604
    epoch 037, train_loss: 0.4612, train_acc: 0.82, valid_loss: 0.6881, valid_acc: 0.57, lr: 4.9292e-04, et: 0.0615
    epoch 038, train_loss: 0.4527, train_acc: 0.83, valid_loss: 0.6851, valid_acc: 0.57, lr: 4.9255e-04, et: 0.0608
    epoch 039, train_loss: 0.4350, train_acc: 0.82, valid_loss: 0.6818, valid_acc: 0.57, lr: 4.9216e-04, et: 0.0610
    epoch 040, train_loss: 0.4099, train_acc: 0.82, valid_loss: 0.6836, valid_acc: 0.55, lr: 4.9177e-04, et: 0.0607
    epoch 041, train_loss: 0.4221, train_acc: 0.82, valid_loss: 0.6777, valid_acc: 0.60, lr: 4.9136e-04, et: 0.0607
    epoch 042, train_loss: 0.4111, train_acc: 0.83, valid_loss: 0.6817, valid_acc: 0.57, lr: 4.9095e-04, et: 0.0607
    epoch 043, train_loss: 0.4289, train_acc: 0.78, valid_loss: 0.6825, valid_acc: 0.55, lr: 4.9053e-04, et: 0.0606
    epoch 044, train_loss: 0.3554, train_acc: 0.89, valid_loss: 0.6822, valid_acc: 0.60, lr: 4.9009e-04, et: 0.0606
    epoch 045, train_loss: 0.3693, train_acc: 0.89, valid_loss: 0.6854, valid_acc: 0.55, lr: 4.8965e-04, et: 0.0619
    epoch 046, train_loss: 0.3929, train_acc: 0.80, valid_loss: 0.6901, valid_acc: 0.57, lr: 4.8920e-04, et: 0.0606
    epoch 047, train_loss: 0.4205, train_acc: 0.89, valid_loss: 0.6856, valid_acc: 0.57, lr: 4.8874e-04, et: 0.0617
    epoch 048, train_loss: 0.3476, train_acc: 0.88, valid_loss: 0.6829, valid_acc: 0.60, lr: 4.8827e-04, et: 0.0632
    epoch 049, train_loss: 0.3805, train_acc: 0.89, valid_loss: 0.6844, valid_acc: 0.57, lr: 4.8779e-04, et: 0.0632
    epoch 050, train_loss: 0.3386, train_acc: 0.88, valid_loss: 0.6836, valid_acc: 0.57, lr: 4.8730e-04, et: 0.0622
    epoch 051, train_loss: 0.3670, train_acc: 0.88, valid_loss: 0.6805, valid_acc: 0.55, lr: 4.8680e-04, et: 0.0633
    epoch 052, train_loss: 0.3289, train_acc: 0.93, valid_loss: 0.6686, valid_acc: 0.57, lr: 4.8629e-04, et: 0.0636
    epoch 053, train_loss: 0.3614, train_acc: 0.88, valid_loss: 0.6626, valid_acc: 0.62, lr: 4.8578e-04, et: 0.0646, checkpoint saved
    epoch 054, train_loss: 0.3258, train_acc: 0.89, valid_loss: 0.6667, valid_acc: 0.62, lr: 4.8525e-04, et: 0.0641
    epoch 055, train_loss: 0.3692, train_acc: 0.88, valid_loss: 0.6724, valid_acc: 0.62, lr: 4.8471e-04, et: 0.0639
    epoch 056, train_loss: 0.3319, train_acc: 0.93, valid_loss: 0.6648, valid_acc: 0.62, lr: 4.8417e-04, et: 0.0631
    epoch 057, train_loss: 0.3155, train_acc: 0.84, valid_loss: 0.6504, valid_acc: 0.65, lr: 4.8362e-04, et: 0.0631, checkpoint saved
    epoch 058, train_loss: 0.2880, train_acc: 0.93, valid_loss: 0.6552, valid_acc: 0.60, lr: 4.8305e-04, et: 0.0632
    epoch 059, train_loss: 0.3115, train_acc: 0.88, valid_loss: 0.6422, valid_acc: 0.62, lr: 4.8248e-04, et: 0.0634, checkpoint saved
    epoch 060, train_loss: 0.2572, train_acc: 0.90, valid_loss: 0.6202, valid_acc: 0.62, lr: 4.8190e-04, et: 0.0624, checkpoint saved
    epoch 061, train_loss: 0.2708, train_acc: 0.93, valid_loss: 0.6021, valid_acc: 0.65, lr: 4.8131e-04, et: 0.0645, checkpoint saved
    epoch 062, train_loss: 0.2782, train_acc: 0.91, valid_loss: 0.5950, valid_acc: 0.65, lr: 4.8071e-04, et: 0.0614, checkpoint saved
    epoch 063, train_loss: 0.2766, train_acc: 0.90, valid_loss: 0.5877, valid_acc: 0.70, lr: 4.8010e-04, et: 0.0608, checkpoint saved
    epoch 064, train_loss: 0.3059, train_acc: 0.89, valid_loss: 0.5932, valid_acc: 0.60, lr: 4.7948e-04, et: 0.0615
    epoch 065, train_loss: 0.2851, train_acc: 0.92, valid_loss: 0.5710, valid_acc: 0.70, lr: 4.7885e-04, et: 0.0607, checkpoint saved
    epoch 066, train_loss: 0.2151, train_acc: 0.95, valid_loss: 0.5432, valid_acc: 0.75, lr: 4.7822e-04, et: 0.0607, checkpoint saved
    epoch 067, train_loss: 0.2484, train_acc: 0.92, valid_loss: 0.5274, valid_acc: 0.75, lr: 4.7757e-04, et: 0.0611, checkpoint saved
    epoch 068, train_loss: 0.2155, train_acc: 0.97, valid_loss: 0.5066, valid_acc: 0.75, lr: 4.7692e-04, et: 0.0597, checkpoint saved
    epoch 069, train_loss: 0.2332, train_acc: 0.93, valid_loss: 0.5041, valid_acc: 0.75, lr: 4.7625e-04, et: 0.0611, checkpoint saved
    epoch 070, train_loss: 0.2506, train_acc: 0.92, valid_loss: 0.5072, valid_acc: 0.72, lr: 4.7558e-04, et: 0.0600
    epoch 071, train_loss: 0.2285, train_acc: 0.94, valid_loss: 0.4766, valid_acc: 0.78, lr: 4.7490e-04, et: 0.0604, checkpoint saved
    epoch 072, train_loss: 0.2241, train_acc: 0.93, valid_loss: 0.4608, valid_acc: 0.80, lr: 4.7421e-04, et: 0.0609, checkpoint saved
    epoch 073, train_loss: 0.1946, train_acc: 0.90, valid_loss: 0.4505, valid_acc: 0.80, lr: 4.7351e-04, et: 0.0608, checkpoint saved
    epoch 074, train_loss: 0.2009, train_acc: 0.95, valid_loss: 0.4485, valid_acc: 0.80, lr: 4.7281e-04, et: 0.0616, checkpoint saved
    epoch 075, train_loss: 0.2072, train_acc: 0.93, valid_loss: 0.4441, valid_acc: 0.80, lr: 4.7209e-04, et: 0.0605, checkpoint saved
    epoch 076, train_loss: 0.2156, train_acc: 0.92, valid_loss: 0.4271, valid_acc: 0.82, lr: 4.7137e-04, et: 0.0608, checkpoint saved
    epoch 077, train_loss: 0.1885, train_acc: 0.92, valid_loss: 0.4286, valid_acc: 0.80, lr: 4.7063e-04, et: 0.0627
    epoch 078, train_loss: 0.2088, train_acc: 0.90, valid_loss: 0.4280, valid_acc: 0.80, lr: 4.6989e-04, et: 0.0615
    epoch 079, train_loss: 0.2066, train_acc: 0.93, valid_loss: 0.4296, valid_acc: 0.80, lr: 4.6914e-04, et: 0.0614
    epoch 080, train_loss: 0.1776, train_acc: 0.97, valid_loss: 0.4304, valid_acc: 0.82, lr: 4.6838e-04, et: 0.0608
    epoch 081, train_loss: 0.1735, train_acc: 0.95, valid_loss: 0.4554, valid_acc: 0.80, lr: 4.6761e-04, et: 0.0615
    epoch 082, train_loss: 0.1546, train_acc: 0.97, valid_loss: 0.4370, valid_acc: 0.82, lr: 4.6683e-04, et: 0.0619
    epoch 083, train_loss: 0.1750, train_acc: 0.95, valid_loss: 0.4440, valid_acc: 0.82, lr: 4.6605e-04, et: 0.0615
    epoch 084, train_loss: 0.1635, train_acc: 0.96, valid_loss: 0.4408, valid_acc: 0.78, lr: 4.6526e-04, et: 0.0598
    epoch 085, train_loss: 0.1872, train_acc: 0.93, valid_loss: 0.4512, valid_acc: 0.82, lr: 4.6445e-04, et: 0.0597
    epoch 086, train_loss: 0.1397, train_acc: 0.96, valid_loss: 0.4507, valid_acc: 0.85, lr: 4.6364e-04, et: 0.0601
    epoch 087, train_loss: 0.1872, train_acc: 0.93, valid_loss: 0.4434, valid_acc: 0.78, lr: 4.6282e-04, et: 0.0604
    epoch 088, train_loss: 0.2085, train_acc: 0.92, valid_loss: 0.4473, valid_acc: 0.80, lr: 4.6200e-04, et: 0.0597
    epoch 089, train_loss: 0.1489, train_acc: 0.96, valid_loss: 0.4467, valid_acc: 0.80, lr: 4.6116e-04, et: 0.0596
    epoch 090, train_loss: 0.1448, train_acc: 0.97, valid_loss: 0.4417, valid_acc: 0.82, lr: 4.6032e-04, et: 0.0597
    epoch 091, train_loss: 0.1283, train_acc: 0.97, valid_loss: 0.4416, valid_acc: 0.80, lr: 4.5946e-04, et: 0.0609
    epoch 092, train_loss: 0.2164, train_acc: 0.92, valid_loss: 0.4442, valid_acc: 0.82, lr: 4.5860e-04, et: 0.0596
    epoch 093, train_loss: 0.1698, train_acc: 0.93, valid_loss: 0.4430, valid_acc: 0.82, lr: 4.5773e-04, et: 0.0608
    epoch 094, train_loss: 0.1412, train_acc: 0.96, valid_loss: 0.4486, valid_acc: 0.82, lr: 4.5686e-04, et: 0.0608
    epoch 095, train_loss: 0.1821, train_acc: 0.93, valid_loss: 0.4440, valid_acc: 0.80, lr: 4.5597e-04, et: 0.0629
    epoch 096, train_loss: 0.1335, train_acc: 0.97, valid_loss: 0.4523, valid_acc: 0.80, lr: 4.5508e-04, et: 0.0631
    epoch 097, train_loss: 0.1261, train_acc: 0.97, valid_loss: 0.4561, valid_acc: 0.82, lr: 4.5418e-04, et: 0.0611
    epoch 098, train_loss: 0.1136, train_acc: 0.97, valid_loss: 0.4369, valid_acc: 0.82, lr: 4.5327e-04, et: 0.0608
    epoch 099, train_loss: 0.1164, train_acc: 0.95, valid_loss: 0.4555, valid_acc: 0.80, lr: 4.5235e-04, et: 0.0635
    epoch 100, train_loss: 0.1327, train_acc: 0.96, valid_loss: 0.4294, valid_acc: 0.82, lr: 4.5142e-04, et: 0.0650
    epoch 101, train_loss: 0.1430, train_acc: 0.94, valid_loss: 0.4340, valid_acc: 0.82, lr: 4.5049e-04, et: 0.0631
    epoch 102, train_loss: 0.1215, train_acc: 0.96, valid_loss: 0.4290, valid_acc: 0.82, lr: 4.4955e-04, et: 0.0623
    epoch 103, train_loss: 0.1466, train_acc: 0.94, valid_loss: 0.4292, valid_acc: 0.82, lr: 4.4860e-04, et: 0.0629
    epoch 104, train_loss: 0.1065, train_acc: 0.97, valid_loss: 0.4220, valid_acc: 0.80, lr: 4.4764e-04, et: 0.0630, checkpoint saved
    epoch 105, train_loss: 0.1152, train_acc: 0.97, valid_loss: 0.4182, valid_acc: 0.82, lr: 4.4668e-04, et: 0.0640, checkpoint saved
    epoch 106, train_loss: 0.1094, train_acc: 0.97, valid_loss: 0.4026, valid_acc: 0.82, lr: 4.4571e-04, et: 0.0629, checkpoint saved
    epoch 107, train_loss: 0.1004, train_acc: 0.97, valid_loss: 0.4073, valid_acc: 0.82, lr: 4.4473e-04, et: 0.0627
    epoch 108, train_loss: 0.0991, train_acc: 0.97, valid_loss: 0.4167, valid_acc: 0.85, lr: 4.4374e-04, et: 0.0646
    epoch 109, train_loss: 0.1230, train_acc: 0.95, valid_loss: 0.4170, valid_acc: 0.82, lr: 4.4274e-04, et: 0.0635
    epoch 110, train_loss: 0.1295, train_acc: 0.96, valid_loss: 0.4016, valid_acc: 0.82, lr: 4.4174e-04, et: 0.0633, checkpoint saved
    epoch 111, train_loss: 0.0946, train_acc: 0.97, valid_loss: 0.3944, valid_acc: 0.85, lr: 4.4073e-04, et: 0.0632, checkpoint saved
    epoch 112, train_loss: 0.1458, train_acc: 0.95, valid_loss: 0.3933, valid_acc: 0.85, lr: 4.3971e-04, et: 0.0632, checkpoint saved
    epoch 113, train_loss: 0.1361, train_acc: 0.97, valid_loss: 0.3808, valid_acc: 0.85, lr: 4.3869e-04, et: 0.0617, checkpoint saved
    epoch 114, train_loss: 0.1325, train_acc: 0.96, valid_loss: 0.4017, valid_acc: 0.82, lr: 4.3765e-04, et: 0.0607
    epoch 115, train_loss: 0.1557, train_acc: 0.93, valid_loss: 0.3739, valid_acc: 0.82, lr: 4.3661e-04, et: 0.0610, checkpoint saved
    epoch 116, train_loss: 0.0883, train_acc: 0.98, valid_loss: 0.3747, valid_acc: 0.82, lr: 4.3556e-04, et: 0.0657
    epoch 117, train_loss: 0.1269, train_acc: 0.96, valid_loss: 0.4115, valid_acc: 0.80, lr: 4.3451e-04, et: 0.0644
    epoch 118, train_loss: 0.1183, train_acc: 0.96, valid_loss: 0.3993, valid_acc: 0.82, lr: 4.3345e-04, et: 0.0640
    epoch 119, train_loss: 0.1253, train_acc: 0.97, valid_loss: 0.3969, valid_acc: 0.82, lr: 4.3238e-04, et: 0.0630
    epoch 120, train_loss: 0.1132, train_acc: 0.97, valid_loss: 0.3956, valid_acc: 0.82, lr: 4.3130e-04, et: 0.0619
    epoch 121, train_loss: 0.1955, train_acc: 0.91, valid_loss: 0.3920, valid_acc: 0.82, lr: 4.3022e-04, et: 0.0625
    epoch 122, train_loss: 0.1235, train_acc: 0.97, valid_loss: 0.4008, valid_acc: 0.82, lr: 4.2913e-04, et: 0.0788
    epoch 123, train_loss: 0.1113, train_acc: 0.95, valid_loss: 0.3868, valid_acc: 0.82, lr: 4.2803e-04, et: 0.0703
    epoch 124, train_loss: 0.0636, train_acc: 0.98, valid_loss: 0.3810, valid_acc: 0.85, lr: 4.2692e-04, et: 0.0643
    epoch 125, train_loss: 0.1030, train_acc: 0.96, valid_loss: 0.3772, valid_acc: 0.85, lr: 4.2581e-04, et: 0.0665
    epoch 126, train_loss: 0.1122, train_acc: 0.95, valid_loss: 0.3855, valid_acc: 0.82, lr: 4.2469e-04, et: 0.0720
    epoch 127, train_loss: 0.0859, train_acc: 0.98, valid_loss: 0.3833, valid_acc: 0.82, lr: 4.2357e-04, et: 0.0663
    epoch 128, train_loss: 0.1038, train_acc: 0.96, valid_loss: 0.3789, valid_acc: 0.82, lr: 4.2243e-04, et: 0.0652
    epoch 129, train_loss: 0.1138, train_acc: 0.97, valid_loss: 0.3737, valid_acc: 0.88, lr: 4.2129e-04, et: 0.0642, checkpoint saved
    epoch 130, train_loss: 0.0965, train_acc: 0.95, valid_loss: 0.3687, valid_acc: 0.88, lr: 4.2015e-04, et: 0.0644, checkpoint saved
    epoch 131, train_loss: 0.1291, train_acc: 0.94, valid_loss: 0.3673, valid_acc: 0.85, lr: 4.1900e-04, et: 0.0634, checkpoint saved
    epoch 132, train_loss: 0.1411, train_acc: 0.95, valid_loss: 0.3680, valid_acc: 0.85, lr: 4.1784e-04, et: 0.0635
    epoch 133, train_loss: 0.1317, train_acc: 0.94, valid_loss: 0.3733, valid_acc: 0.88, lr: 4.1667e-04, et: 0.0666
    epoch 134, train_loss: 0.1039, train_acc: 0.95, valid_loss: 0.3819, valid_acc: 0.85, lr: 4.1550e-04, et: 0.0648
    epoch 135, train_loss: 0.1405, train_acc: 0.95, valid_loss: 0.3624, valid_acc: 0.85, lr: 4.1432e-04, et: 0.0643, checkpoint saved
    epoch 136, train_loss: 0.0907, train_acc: 0.97, valid_loss: 0.3554, valid_acc: 0.85, lr: 4.1313e-04, et: 0.0647, checkpoint saved
    epoch 137, train_loss: 0.0681, train_acc: 0.98, valid_loss: 0.3517, valid_acc: 0.85, lr: 4.1194e-04, et: 0.0635, checkpoint saved
    epoch 138, train_loss: 0.1010, train_acc: 0.95, valid_loss: 0.3552, valid_acc: 0.85, lr: 4.1074e-04, et: 0.0628
    epoch 139, train_loss: 0.1474, train_acc: 0.94, valid_loss: 0.3629, valid_acc: 0.85, lr: 4.0954e-04, et: 0.0631
    epoch 140, train_loss: 0.0999, train_acc: 0.95, valid_loss: 0.3663, valid_acc: 0.85, lr: 4.0833e-04, et: 0.0631
    epoch 141, train_loss: 0.1267, train_acc: 0.97, valid_loss: 0.3591, valid_acc: 0.85, lr: 4.0711e-04, et: 0.0637
    epoch 142, train_loss: 0.1306, train_acc: 0.97, valid_loss: 0.3602, valid_acc: 0.85, lr: 4.0589e-04, et: 0.0633
    epoch 143, train_loss: 0.1017, train_acc: 0.97, valid_loss: 0.3727, valid_acc: 0.82, lr: 4.0466e-04, et: 0.0620
    epoch 144, train_loss: 0.1217, train_acc: 0.95, valid_loss: 0.3620, valid_acc: 0.85, lr: 4.0342e-04, et: 0.0624
    epoch 145, train_loss: 0.0775, train_acc: 0.97, valid_loss: 0.3592, valid_acc: 0.85, lr: 4.0218e-04, et: 0.0618
    epoch 146, train_loss: 0.0762, train_acc: 0.98, valid_loss: 0.3556, valid_acc: 0.85, lr: 4.0093e-04, et: 0.0613
    epoch 147, train_loss: 0.0836, train_acc: 0.98, valid_loss: 0.3511, valid_acc: 0.85, lr: 3.9968e-04, et: 0.0613, checkpoint saved
    epoch 148, train_loss: 0.1474, train_acc: 0.94, valid_loss: 0.3809, valid_acc: 0.82, lr: 3.9842e-04, et: 0.0597
    epoch 149, train_loss: 0.0590, train_acc: 0.99, valid_loss: 0.3853, valid_acc: 0.82, lr: 3.9715e-04, et: 0.0598
    epoch 150, train_loss: 0.0842, train_acc: 0.97, valid_loss: 0.3563, valid_acc: 0.85, lr: 3.9588e-04, et: 0.0597
    epoch 151, train_loss: 0.0919, train_acc: 0.97, valid_loss: 0.3545, valid_acc: 0.85, lr: 3.9460e-04, et: 0.0602
    epoch 152, train_loss: 0.0741, train_acc: 0.98, valid_loss: 0.3585, valid_acc: 0.85, lr: 3.9332e-04, et: 0.0607
    epoch 153, train_loss: 0.0915, train_acc: 0.97, valid_loss: 0.3500, valid_acc: 0.85, lr: 3.9203e-04, et: 0.0598, checkpoint saved
    epoch 154, train_loss: 0.0714, train_acc: 0.98, valid_loss: 0.3498, valid_acc: 0.85, lr: 3.9074e-04, et: 0.0598, checkpoint saved
    epoch 155, train_loss: 0.1017, train_acc: 0.96, valid_loss: 0.3560, valid_acc: 0.85, lr: 3.8944e-04, et: 0.0604
    epoch 156, train_loss: 0.0784, train_acc: 0.97, valid_loss: 0.3431, valid_acc: 0.85, lr: 3.8814e-04, et: 0.0603, checkpoint saved
    epoch 157, train_loss: 0.0755, train_acc: 0.97, valid_loss: 0.3391, valid_acc: 0.85, lr: 3.8683e-04, et: 0.0597, checkpoint saved
    epoch 158, train_loss: 0.0673, train_acc: 0.98, valid_loss: 0.3389, valid_acc: 0.85, lr: 3.8551e-04, et: 0.0597, checkpoint saved
    epoch 159, train_loss: 0.0732, train_acc: 0.97, valid_loss: 0.3453, valid_acc: 0.85, lr: 3.8419e-04, et: 0.0597
    epoch 160, train_loss: 0.0798, train_acc: 0.97, valid_loss: 0.3414, valid_acc: 0.85, lr: 3.8286e-04, et: 0.0597
    epoch 161, train_loss: 0.0879, train_acc: 0.97, valid_loss: 0.3337, valid_acc: 0.85, lr: 3.8153e-04, et: 0.0596, checkpoint saved
    epoch 162, train_loss: 0.0694, train_acc: 0.98, valid_loss: 0.3297, valid_acc: 0.85, lr: 3.8019e-04, et: 0.0598, checkpoint saved
    epoch 163, train_loss: 0.0481, train_acc: 1.00, valid_loss: 0.3390, valid_acc: 0.85, lr: 3.7885e-04, et: 0.0597
    epoch 164, train_loss: 0.1049, train_acc: 0.96, valid_loss: 0.3453, valid_acc: 0.85, lr: 3.7751e-04, et: 0.0596
    epoch 165, train_loss: 0.1262, train_acc: 0.96, valid_loss: 0.3411, valid_acc: 0.88, lr: 3.7615e-04, et: 0.0602
    epoch 166, train_loss: 0.0900, train_acc: 0.97, valid_loss: 0.3457, valid_acc: 0.88, lr: 3.7480e-04, et: 0.0598
    epoch 167, train_loss: 0.0701, train_acc: 0.98, valid_loss: 0.3424, valid_acc: 0.85, lr: 3.7344e-04, et: 0.0598
    epoch 168, train_loss: 0.0699, train_acc: 0.97, valid_loss: 0.3345, valid_acc: 0.88, lr: 3.7207e-04, et: 0.0596
    epoch 169, train_loss: 0.0662, train_acc: 0.99, valid_loss: 0.3415, valid_acc: 0.88, lr: 3.7070e-04, et: 0.0596
    epoch 170, train_loss: 0.0605, train_acc: 0.98, valid_loss: 0.3453, valid_acc: 0.88, lr: 3.6932e-04, et: 0.0597
    epoch 171, train_loss: 0.0630, train_acc: 0.97, valid_loss: 0.3435, valid_acc: 0.85, lr: 3.6794e-04, et: 0.0597
    epoch 172, train_loss: 0.0703, train_acc: 0.97, valid_loss: 0.3458, valid_acc: 0.85, lr: 3.6656e-04, et: 0.0698
    epoch 173, train_loss: 0.0701, train_acc: 0.98, valid_loss: 0.3337, valid_acc: 0.85, lr: 3.6517e-04, et: 0.0684
    epoch 174, train_loss: 0.0601, train_acc: 0.97, valid_loss: 0.3226, valid_acc: 0.85, lr: 3.6377e-04, et: 0.0658, checkpoint saved
    epoch 175, train_loss: 0.0512, train_acc: 0.98, valid_loss: 0.3191, valid_acc: 0.85, lr: 3.6237e-04, et: 0.0633, checkpoint saved
    epoch 176, train_loss: 0.0727, train_acc: 0.98, valid_loss: 0.3216, valid_acc: 0.88, lr: 3.6097e-04, et: 0.0625
    epoch 177, train_loss: 0.0755, train_acc: 0.98, valid_loss: 0.3327, valid_acc: 0.88, lr: 3.5956e-04, et: 0.0627
    epoch 178, train_loss: 0.0739, train_acc: 0.97, valid_loss: 0.3355, valid_acc: 0.88, lr: 3.5815e-04, et: 0.0625
    epoch 179, train_loss: 0.0559, train_acc: 0.99, valid_loss: 0.3307, valid_acc: 0.88, lr: 3.5673e-04, et: 0.0606
    epoch 180, train_loss: 0.1155, train_acc: 0.95, valid_loss: 0.3071, valid_acc: 0.88, lr: 3.5531e-04, et: 0.0603, checkpoint saved
    epoch 181, train_loss: 0.0888, train_acc: 0.97, valid_loss: 0.3166, valid_acc: 0.88, lr: 3.5389e-04, et: 0.0716
    epoch 182, train_loss: 0.0617, train_acc: 0.97, valid_loss: 0.3205, valid_acc: 0.88, lr: 3.5246e-04, et: 0.0698
    epoch 183, train_loss: 0.0868, train_acc: 0.97, valid_loss: 0.3196, valid_acc: 0.88, lr: 3.5103e-04, et: 0.0714
    epoch 184, train_loss: 0.0844, train_acc: 0.96, valid_loss: 0.3102, valid_acc: 0.85, lr: 3.4959e-04, et: 0.0645
    epoch 185, train_loss: 0.0497, train_acc: 0.99, valid_loss: 0.3045, valid_acc: 0.88, lr: 3.4815e-04, et: 0.0622, checkpoint saved
    epoch 186, train_loss: 0.0618, train_acc: 0.99, valid_loss: 0.3019, valid_acc: 0.85, lr: 3.4670e-04, et: 0.0624, checkpoint saved
    epoch 187, train_loss: 0.0848, train_acc: 0.98, valid_loss: 0.3075, valid_acc: 0.85, lr: 3.4525e-04, et: 0.0624
    epoch 188, train_loss: 0.0519, train_acc: 0.98, valid_loss: 0.3104, valid_acc: 0.85, lr: 3.4380e-04, et: 0.0608
    epoch 189, train_loss: 0.0591, train_acc: 0.99, valid_loss: 0.3105, valid_acc: 0.88, lr: 3.4235e-04, et: 0.0606
    epoch 190, train_loss: 0.0979, train_acc: 0.97, valid_loss: 0.3207, valid_acc: 0.88, lr: 3.4089e-04, et: 0.0620
    epoch 191, train_loss: 0.0674, train_acc: 0.97, valid_loss: 0.3158, valid_acc: 0.88, lr: 3.3942e-04, et: 0.0614
    epoch 192, train_loss: 0.0951, train_acc: 0.97, valid_loss: 0.3231, valid_acc: 0.88, lr: 3.3796e-04, et: 0.0607
    epoch 193, train_loss: 0.0766, train_acc: 0.98, valid_loss: 0.3239, valid_acc: 0.88, lr: 3.3649e-04, et: 0.0607
    epoch 194, train_loss: 0.0607, train_acc: 0.97, valid_loss: 0.3274, valid_acc: 0.85, lr: 3.3502e-04, et: 0.0606
    epoch 195, train_loss: 0.1083, train_acc: 0.97, valid_loss: 0.3343, valid_acc: 0.85, lr: 3.3354e-04, et: 0.0604
    epoch 196, train_loss: 0.0923, train_acc: 0.97, valid_loss: 0.3223, valid_acc: 0.88, lr: 3.3206e-04, et: 0.0605
    epoch 197, train_loss: 0.0593, train_acc: 0.99, valid_loss: 0.3094, valid_acc: 0.88, lr: 3.3058e-04, et: 0.0605
    epoch 198, train_loss: 0.0444, train_acc: 0.99, valid_loss: 0.3032, valid_acc: 0.88, lr: 3.2909e-04, et: 0.0604
    epoch 199, train_loss: 0.1106, train_acc: 0.97, valid_loss: 0.3136, valid_acc: 0.88, lr: 3.2760e-04, et: 0.0604
    epoch 200, train_loss: 0.0546, train_acc: 0.99, valid_loss: 0.3170, valid_acc: 0.88, lr: 3.2611e-04, et: 0.0611
    epoch 201, train_loss: 0.0839, train_acc: 0.96, valid_loss: 0.3126, valid_acc: 0.88, lr: 3.2461e-04, et: 0.0597
    epoch 202, train_loss: 0.0653, train_acc: 0.97, valid_loss: 0.3047, valid_acc: 0.85, lr: 3.2311e-04, et: 0.0598
    epoch 203, train_loss: 0.0879, train_acc: 0.96, valid_loss: 0.3072, valid_acc: 0.85, lr: 3.2161e-04, et: 0.0597
    epoch 204, train_loss: 0.0492, train_acc: 1.00, valid_loss: 0.3029, valid_acc: 0.85, lr: 3.2011e-04, et: 0.0604
    epoch 205, train_loss: 0.0440, train_acc: 0.99, valid_loss: 0.2962, valid_acc: 0.85, lr: 3.1860e-04, et: 0.0598, checkpoint saved
    epoch 206, train_loss: 0.0684, train_acc: 0.97, valid_loss: 0.3071, valid_acc: 0.85, lr: 3.1709e-04, et: 0.0597
    epoch 207, train_loss: 0.0704, train_acc: 0.99, valid_loss: 0.3117, valid_acc: 0.88, lr: 3.1558e-04, et: 0.0597
    epoch 208, train_loss: 0.0340, train_acc: 0.99, valid_loss: 0.3062, valid_acc: 0.85, lr: 3.1407e-04, et: 0.0596
    epoch 209, train_loss: 0.0753, train_acc: 0.96, valid_loss: 0.2984, valid_acc: 0.85, lr: 3.1255e-04, et: 0.0597
    epoch 210, train_loss: 0.0627, train_acc: 0.99, valid_loss: 0.3112, valid_acc: 0.85, lr: 3.1103e-04, et: 0.0603
    epoch 211, train_loss: 0.1101, train_acc: 0.96, valid_loss: 0.3202, valid_acc: 0.85, lr: 3.0951e-04, et: 0.0596
    epoch 212, train_loss: 0.0528, train_acc: 0.97, valid_loss: 0.3232, valid_acc: 0.85, lr: 3.0798e-04, et: 0.0596
    epoch 213, train_loss: 0.0626, train_acc: 0.98, valid_loss: 0.3265, valid_acc: 0.88, lr: 3.0646e-04, et: 0.0599
    epoch 214, train_loss: 0.0830, train_acc: 0.96, valid_loss: 0.3241, valid_acc: 0.88, lr: 3.0493e-04, et: 0.0600
    epoch 215, train_loss: 0.0873, train_acc: 0.96, valid_loss: 0.3076, valid_acc: 0.85, lr: 3.0340e-04, et: 0.0598
    epoch 216, train_loss: 0.0710, train_acc: 0.97, valid_loss: 0.3104, valid_acc: 0.85, lr: 3.0186e-04, et: 0.0596
    epoch 217, train_loss: 0.0783, train_acc: 0.97, valid_loss: 0.3113, valid_acc: 0.85, lr: 3.0033e-04, et: 0.0596
    epoch 218, train_loss: 0.0425, train_acc: 0.99, valid_loss: 0.3132, valid_acc: 0.85, lr: 2.9879e-04, et: 0.0597
    epoch 219, train_loss: 0.0599, train_acc: 0.99, valid_loss: 0.3260, valid_acc: 0.85, lr: 2.9725e-04, et: 0.0597
    epoch 220, train_loss: 0.1257, train_acc: 0.93, valid_loss: 0.3342, valid_acc: 0.88, lr: 2.9571e-04, et: 0.0603
    epoch 221, train_loss: 0.0461, train_acc: 0.99, valid_loss: 0.3415, valid_acc: 0.90, lr: 2.9417e-04, et: 0.0613
    epoch 222, train_loss: 0.1065, train_acc: 0.96, valid_loss: 0.3446, valid_acc: 0.88, lr: 2.9262e-04, et: 0.0599
    epoch 223, train_loss: 0.0772, train_acc: 0.98, valid_loss: 0.3439, valid_acc: 0.90, lr: 2.9108e-04, et: 0.0597
    epoch 224, train_loss: 0.0544, train_acc: 0.98, valid_loss: 0.3376, valid_acc: 0.85, lr: 2.8953e-04, et: 0.0597
    epoch 225, train_loss: 0.1092, train_acc: 0.96, valid_loss: 0.3399, valid_acc: 0.85, lr: 2.8798e-04, et: 0.0596
    epoch 226, train_loss: 0.0506, train_acc: 0.99, valid_loss: 0.3407, valid_acc: 0.88, lr: 2.8643e-04, et: 0.0596
    epoch 227, train_loss: 0.0571, train_acc: 0.99, valid_loss: 0.3318, valid_acc: 0.85, lr: 2.8488e-04, et: 0.0601
    epoch 228, train_loss: 0.0748, train_acc: 0.97, valid_loss: 0.3446, valid_acc: 0.88, lr: 2.8333e-04, et: 0.0607
    epoch 229, train_loss: 0.0530, train_acc: 0.99, valid_loss: 0.3407, valid_acc: 0.88, lr: 2.8177e-04, et: 0.0605
    epoch 230, train_loss: 0.0397, train_acc: 1.00, valid_loss: 0.3376, valid_acc: 0.88, lr: 2.8021e-04, et: 0.0610
    epoch 231, train_loss: 0.0698, train_acc: 0.97, valid_loss: 0.3291, valid_acc: 0.88, lr: 2.7866e-04, et: 0.0605
    epoch 232, train_loss: 0.1207, train_acc: 0.95, valid_loss: 0.3293, valid_acc: 0.85, lr: 2.7710e-04, et: 0.0605
    epoch 233, train_loss: 0.0458, train_acc: 0.99, valid_loss: 0.3239, valid_acc: 0.85, lr: 2.7554e-04, et: 0.0605
    epoch 234, train_loss: 0.0734, train_acc: 0.97, valid_loss: 0.3291, valid_acc: 0.88, lr: 2.7398e-04, et: 0.0605
    epoch 235, train_loss: 0.0621, train_acc: 0.98, valid_loss: 0.3235, valid_acc: 0.88, lr: 2.7242e-04, et: 0.0605
    epoch 236, train_loss: 0.0285, train_acc: 1.00, valid_loss: 0.3183, valid_acc: 0.88, lr: 2.7086e-04, et: 0.0605
    epoch 237, train_loss: 0.0544, train_acc: 0.98, valid_loss: 0.3163, valid_acc: 0.88, lr: 2.6929e-04, et: 0.0610
    epoch 238, train_loss: 0.1008, train_acc: 0.95, valid_loss: 0.3090, valid_acc: 0.85, lr: 2.6773e-04, et: 0.0605
    epoch 239, train_loss: 0.0558, train_acc: 0.97, valid_loss: 0.3098, valid_acc: 0.85, lr: 2.6617e-04, et: 0.0605
    epoch 240, train_loss: 0.1244, train_acc: 0.93, valid_loss: 0.3134, valid_acc: 0.85, lr: 2.6460e-04, et: 0.0613
    epoch 241, train_loss: 0.0565, train_acc: 0.99, valid_loss: 0.3129, valid_acc: 0.85, lr: 2.6304e-04, et: 0.0606
    epoch 242, train_loss: 0.0566, train_acc: 0.97, valid_loss: 0.3101, valid_acc: 0.85, lr: 2.6147e-04, et: 0.0605
    epoch 243, train_loss: 0.1618, train_acc: 0.94, valid_loss: 0.3322, valid_acc: 0.88, lr: 2.5990e-04, et: 0.0605
    epoch 244, train_loss: 0.0928, train_acc: 0.97, valid_loss: 0.3277, valid_acc: 0.88, lr: 2.5834e-04, et: 0.0606
    epoch 245, train_loss: 0.0764, train_acc: 0.96, valid_loss: 0.3170, valid_acc: 0.85, lr: 2.5677e-04, et: 0.0606
    epoch 246, train_loss: 0.0486, train_acc: 0.99, valid_loss: 0.3183, valid_acc: 0.85, lr: 2.5520e-04, et: 0.0607
    epoch 247, train_loss: 0.0341, train_acc: 0.99, valid_loss: 0.3169, valid_acc: 0.85, lr: 2.5364e-04, et: 0.0606
    epoch 248, train_loss: 0.0379, train_acc: 0.99, valid_loss: 0.3266, valid_acc: 0.85, lr: 2.5207e-04, et: 0.0606
    epoch 249, train_loss: 0.0581, train_acc: 0.98, valid_loss: 0.3305, valid_acc: 0.85, lr: 2.5050e-04, et: 0.0605
    epoch 250, train_loss: 0.0403, train_acc: 0.99, valid_loss: 0.3338, valid_acc: 0.85, lr: 2.4893e-04, et: 0.0617
    epoch 251, train_loss: 0.0605, train_acc: 0.98, valid_loss: 0.3244, valid_acc: 0.85, lr: 2.4736e-04, et: 0.0605
    epoch 252, train_loss: 0.0536, train_acc: 0.99, valid_loss: 0.3202, valid_acc: 0.85, lr: 2.4580e-04, et: 0.0604
    epoch 253, train_loss: 0.1111, train_acc: 0.95, valid_loss: 0.3185, valid_acc: 0.85, lr: 2.4423e-04, et: 0.0606
    epoch 254, train_loss: 0.0404, train_acc: 0.99, valid_loss: 0.3265, valid_acc: 0.85, lr: 2.4266e-04, et: 0.0611
    epoch 255, train_loss: 0.0483, train_acc: 0.97, valid_loss: 0.3272, valid_acc: 0.85, lr: 2.4110e-04, et: 0.0607
    epoch 256, train_loss: 0.0569, train_acc: 0.97, valid_loss: 0.3286, valid_acc: 0.85, lr: 2.3953e-04, et: 0.0605
    epoch 257, train_loss: 0.0397, train_acc: 0.99, valid_loss: 0.3326, valid_acc: 0.85, lr: 2.3796e-04, et: 0.0605
    epoch 258, train_loss: 0.0439, train_acc: 0.98, valid_loss: 0.3345, valid_acc: 0.85, lr: 2.3640e-04, et: 0.0606
    epoch 259, train_loss: 0.1165, train_acc: 0.95, valid_loss: 0.3320, valid_acc: 0.85, lr: 2.3483e-04, et: 0.0612
    epoch 260, train_loss: 0.1107, train_acc: 0.94, valid_loss: 0.3275, valid_acc: 0.85, lr: 2.3327e-04, et: 0.0607
    epoch 261, train_loss: 0.0664, train_acc: 0.98, valid_loss: 0.3197, valid_acc: 0.85, lr: 2.3171e-04, et: 0.0606
    epoch 262, train_loss: 0.0932, train_acc: 0.97, valid_loss: 0.3169, valid_acc: 0.85, lr: 2.3014e-04, et: 0.0606
    epoch 263, train_loss: 0.0510, train_acc: 0.98, valid_loss: 0.3186, valid_acc: 0.85, lr: 2.2858e-04, et: 0.0607
    epoch 264, train_loss: 0.0651, train_acc: 0.97, valid_loss: 0.3180, valid_acc: 0.85, lr: 2.2702e-04, et: 0.0606
    epoch 265, train_loss: 0.0479, train_acc: 0.99, valid_loss: 0.3202, valid_acc: 0.85, lr: 2.2546e-04, et: 0.0605
    epoch 266, train_loss: 0.0446, train_acc: 0.98, valid_loss: 0.3275, valid_acc: 0.85, lr: 2.2390e-04, et: 0.0604
    epoch 267, train_loss: 0.1088, train_acc: 0.96, valid_loss: 0.3239, valid_acc: 0.85, lr: 2.2234e-04, et: 0.0607
    epoch 268, train_loss: 0.0911, train_acc: 0.97, valid_loss: 0.3244, valid_acc: 0.85, lr: 2.2079e-04, et: 0.0607
    epoch 269, train_loss: 0.0752, train_acc: 0.98, valid_loss: 0.3197, valid_acc: 0.85, lr: 2.1923e-04, et: 0.0611
    epoch 270, train_loss: 0.0392, train_acc: 1.00, valid_loss: 0.3214, valid_acc: 0.88, lr: 2.1767e-04, et: 0.0609
    epoch 271, train_loss: 0.0293, train_acc: 1.00, valid_loss: 0.3158, valid_acc: 0.85, lr: 2.1612e-04, et: 0.0605
    epoch 272, train_loss: 0.0656, train_acc: 0.97, valid_loss: 0.3145, valid_acc: 0.85, lr: 2.1457e-04, et: 0.0606
    epoch 273, train_loss: 0.0516, train_acc: 0.98, valid_loss: 0.3101, valid_acc: 0.88, lr: 2.1302e-04, et: 0.0606
    epoch 274, train_loss: 0.0726, train_acc: 0.97, valid_loss: 0.3098, valid_acc: 0.85, lr: 2.1147e-04, et: 0.0605
    epoch 275, train_loss: 0.0463, train_acc: 0.98, valid_loss: 0.3086, valid_acc: 0.85, lr: 2.0992e-04, et: 0.0605
    epoch 276, train_loss: 0.0881, train_acc: 0.96, valid_loss: 0.3017, valid_acc: 0.85, lr: 2.0838e-04, et: 0.0606
    epoch 277, train_loss: 0.0487, train_acc: 0.98, valid_loss: 0.3099, valid_acc: 0.85, lr: 2.0683e-04, et: 0.0605
    epoch 278, train_loss: 0.0469, train_acc: 0.99, valid_loss: 0.3114, valid_acc: 0.85, lr: 2.0529e-04, et: 0.0604
    epoch 279, train_loss: 0.0355, train_acc: 0.99, valid_loss: 0.3113, valid_acc: 0.85, lr: 2.0375e-04, et: 0.0610
    epoch 280, train_loss: 0.0613, train_acc: 0.98, valid_loss: 0.3107, valid_acc: 0.85, lr: 2.0221e-04, et: 0.0605
    Early stopping was triggered: epoch #281
    Elapsed Time: 17.50s




.. GENERATED FROM PYTHON SOURCE LINES 228-230

Run the Experiment using Early Stopping with callback_early_stopping
====================================================================

.. GENERATED FROM PYTHON SOURCE LINES 230-293

.. code-block:: Python


    data = torch.load(save_base / "checkpoint" / f"sub-{subject}.pth")
    loss_best = data["loss_best"]

    lr = 5e-4
    weight_decay = 1e-2
    n_epochs = 500
    batch_size = 8
    seed = 42

    criterion = torch.nn.CrossEntropyLoss()
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR
    scheduler_params = {"T_max": n_epochs, "eta_min": 1e-6}
    optimizer = torch.optim.AdamW
    optimizer_params = {"lr": lr, "weight_decay": weight_decay}
    early_stopping = rosoku.utils.EarlyStopping(patience=patience)


    def callback_early_stopping(state):
        # Stop training once the training loss reaches the best
        # validation loss obtained in the first step
        return state["train_loss"] <= loss_best


    results_2nd_step = rosoku.deeplearning(
        items_train=[subject, "R1", "R2", "R3", "R4"],
        items_valid=None,
        items_test=[[subject, "R5", "R6"]],
        callback_load_epochs=functools.partial(
            callback_load_epochs,
            dataset=dataset,
            l_freq=8.0,
            h_freq=30.0,
            order_filter=4,
            tmin=dataset.interval[0] + 0.5,
            tmax=dataset.interval[1],
        ),
        callback_proc_epochs=callback_proc_epochs,
        callback_convert_epochs_to_ndarray=functools.partial(
            convert_epochs_to_ndarray, label_keys=label_keys
        ),
        callback_early_stopping=callback_early_stopping,
        batch_size=batch_size,
        n_epochs=n_epochs,
        criterion=criterion,
        optimizer=optimizer,
        optimizer_params=optimizer_params,
        callback_get_model=callback_get_model,
        scheduler=scheduler,
        scheduler_params=scheduler_params,
        device=device,
        history_fname=(save_base / "history" / f"sub-{subject}_2nd.parquet"),
        checkpoint_fname=(save_base / "checkpoint" / f"sub-{subject}_2nd.pth"),
        samples_fname=(save_base / "samples" / f"sub-{subject}_2nd.parquet"),
        normalization_fname=(save_base / "normalization" / f"sub-{subject}_2nd.msgpack"),
        saliency_map_fname=(save_base / "saliency" / f"sub-{subject}_2nd.msgpack"),
        label_keys=label_keys,
        seed=seed,
        additional_values={"subject": subject},
        use_deterministic_algorithms=True,
        min_delta=0,
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    0it [00:00, ?it/s]    9it [00:00, 41031.23it/s]
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Filtering raw data in 1 contiguous segment
    Setting up band-pass filter from 8 - 30 Hz

    IIR filter parameters
    ---------------------
    Butterworth bandpass zero-phase (two-pass forward and reverse) non-causal filter:
    - Filter order 16 (effective, after forward-backward)
    - Cutoffs at 8.00, 30.00 Hz: -6.02, -6.02 dB

    Used Annotations descriptions: [np.str_('left_hand'), np.str_('right_hand')]
    Ignoring annotation durations and creating fixed-duration epochs around annotation onsets.
    Not setting metadata
    40 matching events found
    No baseline correction applied
    0 projection items activated
    Filtering raw data in 1 contiguous segment
    Setting up band-pass filter from 8 - 30 Hz

    IIR filter parameters
    ---------------------
    Butterworth bandpass zero-phase (two-pass forward and reverse) non-causal filter:
    - Filter order 16 (effective, after forward-backward)
    - Cutoffs at 8.00, 30.00 Hz: -6.02, -6.02 dB

    Used Annotations descriptions: [np.str_('left_hand'), np.str_('right_hand')]
    Ignoring annotation durations and creating fixed-duration epochs around annotation onsets.
    Not setting metadata
    40 matching events found
    No baseline correction applied
    0 projection items activated
    Filtering raw data in 1 contiguous segment
    Setting up band-pass filter from 8 - 30 Hz

    IIR filter parameters
    ---------------------
    Butterworth bandpass zero-phase (two-pass forward and reverse) non-causal filter:
    - Filter order 16 (effective, after forward-backward)
    - Cutoffs at 8.00, 30.00 Hz: -6.02, -6.02 dB

    Used Annotations descriptions: [np.str_('left_hand'), np.str_('right_hand')]
    Ignoring annotation durations and creating fixed-duration epochs around annotation onsets.
    Not setting metadata
    40 matching events found
    No baseline correction applied
    0 projection items activated
    Filtering raw data in 1 contiguous segment
    Setting up band-pass filter from 8 - 30 Hz

    IIR filter parameters
    ---------------------
    Butterworth bandpass zero-phase (two-pass forward and reverse) non-causal filter:
    - Filter order 16 (effective, after forward-backward)
    - Cutoffs at 8.00, 30.00 Hz: -6.02, -6.02 dB

    Used Annotations descriptions: [np.str_('left_hand'), np.str_('right_hand')]
    Ignoring annotation durations and creating fixed-duration epochs around annotation onsets.
    Not setting metadata
    40 matching events found
    No baseline correction applied
    0 projection items activated
    /home/skojima/git/rosoku/examples/02_deeplearning/03_example_2step_early_stopping.py:135: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.
      return mne.concatenate_epochs(epochs_list)
    Using data from preloaded Raw for 40 events and 2305 original time points ...
    0 bad epochs dropped
    Using data from preloaded Raw for 40 events and 2305 original time points ...
    0 bad epochs dropped
    Using data from preloaded Raw for 40 events and 2305 original time points ...
    0 bad epochs dropped
    Using data from preloaded Raw for 40 events and 2305 original time points ...
    0 bad epochs dropped
    Using data from preloaded Raw for 40 events and 2305 original time points ...
    Using data from preloaded Raw for 40 events and 2305 original time points ...
    Using data from preloaded Raw for 40 events and 2305 original time points ...
    Using data from preloaded Raw for 40 events and 2305 original time points ...
    Not setting metadata
    160 matching events found
    No baseline correction applied
    0it [00:00, ?it/s]    9it [00:00, 44410.28it/s]
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Reading 0 ... 230911  =      0.000 ...   450.998 secs...
    Filtering raw data in 1 contiguous segment
    Setting up band-pass filter from 8 - 30 Hz

    IIR filter parameters
    ---------------------
    Butterworth bandpass zero-phase (two-pass forward and reverse) non-causal filter:
    - Filter order 16 (effective, after forward-backward)
    - Cutoffs at 8.00, 30.00 Hz: -6.02, -6.02 dB

    Used Annotations descriptions: [np.str_('left_hand'), np.str_('right_hand')]
    Ignoring annotation durations and creating fixed-duration epochs around annotation onsets.
    Not setting metadata
    40 matching events found
    No baseline correction applied
    0 projection items activated
    Filtering raw data in 1 contiguous segment
    Setting up band-pass filter from 8 - 30 Hz

    IIR filter parameters
    ---------------------
    Butterworth bandpass zero-phase (two-pass forward and reverse) non-causal filter:
    - Filter order 16 (effective, after forward-backward)
    - Cutoffs at 8.00, 30.00 Hz: -6.02, -6.02 dB

    Used Annotations descriptions: [np.str_('left_hand'), np.str_('right_hand')]
    Ignoring annotation durations and creating fixed-duration epochs around annotation onsets.
    Not setting metadata
    40 matching events found
    No baseline correction applied
    0 projection items activated
    /home/skojima/git/rosoku/examples/02_deeplearning/03_example_2step_early_stopping.py:135: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.
      return mne.concatenate_epochs(epochs_list)
    Using data from preloaded Raw for 40 events and 2305 original time points ...
    0 bad epochs dropped
    Using data from preloaded Raw for 40 events and 2305 original time points ...
    0 bad epochs dropped
    Using data from preloaded Raw for 40 events and 2305 original time points ...
    Using data from preloaded Raw for 40 events and 2305 original time points ...
    Not setting metadata
    80 matching events found
    No baseline correction applied
    epoch 000, train_loss: 0.7105, train_acc: 0.52, lr: 5.0000e-04, et: 0.0739, checkpoint saved
    epoch 001, train_loss: 0.6965, train_acc: 0.56, lr: 4.9998e-04, et: 0.0737, checkpoint saved
    epoch 002, train_loss: 0.7049, train_acc: 0.51, lr: 4.9996e-04, et: 0.0732
    epoch 003, train_loss: 0.6524, train_acc: 0.61, lr: 4.9992e-04, et: 0.0731, checkpoint saved
    epoch 004, train_loss: 0.6686, train_acc: 0.57, lr: 4.9988e-04, et: 0.0733
    epoch 005, train_loss: 0.6484, train_acc: 0.66, lr: 4.9982e-04, et: 0.0732, checkpoint saved
    epoch 006, train_loss: 0.6618, train_acc: 0.58, lr: 4.9976e-04, et: 0.0738
    epoch 007, train_loss: 0.6324, train_acc: 0.64, lr: 4.9968e-04, et: 0.0733, checkpoint saved
    epoch 008, train_loss: 0.6132, train_acc: 0.71, lr: 4.9960e-04, et: 0.0733, checkpoint saved
    epoch 009, train_loss: 0.6281, train_acc: 0.66, lr: 4.9951e-04, et: 0.0733
    epoch 010, train_loss: 0.6005, train_acc: 0.69, lr: 4.9940e-04, et: 0.0733, checkpoint saved
    epoch 011, train_loss: 0.5957, train_acc: 0.69, lr: 4.9929e-04, et: 0.0734, checkpoint saved
    epoch 012, train_loss: 0.5753, train_acc: 0.74, lr: 4.9917e-04, et: 0.0731, checkpoint saved
    epoch 013, train_loss: 0.5746, train_acc: 0.69, lr: 4.9904e-04, et: 0.0734, checkpoint saved
    epoch 014, train_loss: 0.5603, train_acc: 0.72, lr: 4.9889e-04, et: 0.0739, checkpoint saved
    epoch 015, train_loss: 0.5558, train_acc: 0.74, lr: 4.9874e-04, et: 0.0728, checkpoint saved
    epoch 016, train_loss: 0.5669, train_acc: 0.71, lr: 4.9858e-04, et: 0.0727
    epoch 017, train_loss: 0.5287, train_acc: 0.80, lr: 4.9841e-04, et: 0.0725, checkpoint saved
    epoch 018, train_loss: 0.5283, train_acc: 0.75, lr: 4.9822e-04, et: 0.0731, checkpoint saved
    epoch 019, train_loss: 0.5251, train_acc: 0.78, lr: 4.9803e-04, et: 0.0730, checkpoint saved
    epoch 020, train_loss: 0.5451, train_acc: 0.74, lr: 4.9783e-04, et: 0.0728
    epoch 021, train_loss: 0.5368, train_acc: 0.74, lr: 4.9762e-04, et: 0.0726
    epoch 022, train_loss: 0.4721, train_acc: 0.84, lr: 4.9740e-04, et: 0.0729, checkpoint saved
    epoch 023, train_loss: 0.5082, train_acc: 0.78, lr: 4.9717e-04, et: 0.0726
    epoch 024, train_loss: 0.4918, train_acc: 0.81, lr: 4.9693e-04, et: 0.0727
    epoch 025, train_loss: 0.5186, train_acc: 0.75, lr: 4.9668e-04, et: 0.0727
    epoch 026, train_loss: 0.4986, train_acc: 0.78, lr: 4.9642e-04, et: 0.0725
    epoch 027, train_loss: 0.5027, train_acc: 0.80, lr: 4.9615e-04, et: 0.0724
    epoch 028, train_loss: 0.4537, train_acc: 0.83, lr: 4.9587e-04, et: 0.0730, checkpoint saved
    epoch 029, train_loss: 0.4795, train_acc: 0.78, lr: 4.9558e-04, et: 0.0726
    epoch 030, train_loss: 0.4866, train_acc: 0.75, lr: 4.9528e-04, et: 0.0729
    epoch 031, train_loss: 0.4843, train_acc: 0.76, lr: 4.9497e-04, et: 0.0723
    epoch 032, train_loss: 0.4709, train_acc: 0.79, lr: 4.9466e-04, et: 0.0725
    epoch 033, train_loss: 0.4606, train_acc: 0.80, lr: 4.9433e-04, et: 0.0725
    epoch 034, train_loss: 0.4201, train_acc: 0.81, lr: 4.9399e-04, et: 0.0724, checkpoint saved
    epoch 035, train_loss: 0.4478, train_acc: 0.84, lr: 4.9364e-04, et: 0.0736
    epoch 036, train_loss: 0.4328, train_acc: 0.79, lr: 4.9329e-04, et: 0.0725
    epoch 037, train_loss: 0.4451, train_acc: 0.82, lr: 4.9292e-04, et: 0.0724
    epoch 038, train_loss: 0.4361, train_acc: 0.78, lr: 4.9255e-04, et: 0.0729
    epoch 039, train_loss: 0.3984, train_acc: 0.88, lr: 4.9216e-04, et: 0.0724, checkpoint saved
    epoch 040, train_loss: 0.4506, train_acc: 0.80, lr: 4.9177e-04, et: 0.0724
    epoch 041, train_loss: 0.3781, train_acc: 0.86, lr: 4.9136e-04, et: 0.0727, checkpoint saved
    epoch 042, train_loss: 0.4048, train_acc: 0.81, lr: 4.9095e-04, et: 0.0727
    epoch 043, train_loss: 0.4138, train_acc: 0.82, lr: 4.9053e-04, et: 0.0725
    epoch 044, train_loss: 0.3629, train_acc: 0.89, lr: 4.9009e-04, et: 0.0724, checkpoint saved
    epoch 045, train_loss: 0.3680, train_acc: 0.87, lr: 4.8965e-04, et: 0.0724
    epoch 046, train_loss: 0.3763, train_acc: 0.86, lr: 4.8920e-04, et: 0.0728
    epoch 047, train_loss: 0.3296, train_acc: 0.89, lr: 4.8874e-04, et: 0.0726, checkpoint saved
    epoch 048, train_loss: 0.3395, train_acc: 0.88, lr: 4.8827e-04, et: 0.0724
    epoch 049, train_loss: 0.3028, train_acc: 0.91, lr: 4.8779e-04, et: 0.0723, checkpoint saved
    epoch 050, train_loss: 0.3573, train_acc: 0.84, lr: 4.8730e-04, et: 0.0725
    epoch 051, train_loss: 0.3531, train_acc: 0.87, lr: 4.8680e-04, et: 0.0723
    epoch 052, train_loss: 0.3275, train_acc: 0.88, lr: 4.8629e-04, et: 0.0723
    epoch 053, train_loss: 0.2517, train_acc: 0.92, lr: 4.8578e-04, et: 0.0738, checkpoint saved
    Early stopping was triggered: epoch #54
    Elapsed Time: 3.98s




.. GENERATED FROM PYTHON SOURCE LINES 294-296

Print Results
=============

.. GENERATED FROM PYTHON SOURCE LINES 296-299

.. code-block:: Python


    print(results_1st_step.to_string())
    print(results_2nd_step.to_string())




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

                  items_train items_valid        items_test  accuracy   model  subject
    0  [10, "R1", "R2", "R3"]  [10, "R4"]  [10, "R5", "R6"]    0.8375  EEGNet       10
                        items_train items_valid        items_test  accuracy   model  subject
    0  [10, "R1", "R2", "R3", "R4"]        null  [10, "R5", "R6"]     0.675  EEGNet       10





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 24.243 seconds)


.. _sphx_glr_download_auto_examples_02_deeplearning_03_example_2step_early_stopping.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 03_example_2step_early_stopping.ipynb <03_example_2step_early_stopping.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 03_example_2step_early_stopping.py <03_example_2step_early_stopping.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 03_example_2step_early_stopping.zip <03_example_2step_early_stopping.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
