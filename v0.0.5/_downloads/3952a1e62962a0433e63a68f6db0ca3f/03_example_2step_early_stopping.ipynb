{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Example 03: 2-step Early Stopping\n\nThis example demonstrates a two-step training strategy using early stopping\nwith ``rosoku`` for deep-learning-based EEG classification.\n\nThe purpose of this example is to illustrate a practical way to estimate a\nreasonable loss target and then use it to control the final training process,\nwhile keeping the training loop unchanged.\n\nThe training is split into two stages:\n\n1. **Step 1 (loss calibration / warm-up)**:\n   A validation set is temporarily introduced and standard early stopping is\n   applied.\n   The goal of this step is to obtain a **rough estimate of an achievable loss\n   value** and a stable training regime.\n   The best loss value and corresponding checkpoint are stored.\n\n2. **Step 2 (final training)**:\n   The model is retrained using a larger training set that also includes the\n   former validation data.\n   Early stopping is now controlled by a custom ``callback_early_stopping``\n   function, which stops training once the training loss reaches the best loss\n   observed in Step 1.\n\nThis strategy is useful when:\n\n- Early stopping is needed, but a permanent validation split is undesirable\n- One wants to **maximize the amount of data used for final training**\n- A loss-based stopping criterion should be **derived empirically**\n  rather than hand-tuned\n\nKey aspects illustrated in this example include:\n\n- Using validation loss only for **loss calibration**\n- Custom early stopping via ``callback_early_stopping``\n- Reusing a loss threshold stored in a checkpoint\n- Implementing advanced stopping logic without modifying the training loop\n\nThis example focuses on the *training strategy* rather than absolute performance,\nand serves as a practical template for flexible early-stopping workflows in\n``rosoku``.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Simon Kojima <simon.kojima@inria.fr>\n#\n# License: BSD (3-clause)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Set Environment Variables for Replicability\nNOTE:\nThis environment variable MUST be set **before importing torch**.\nIt enforces deterministic behavior in CUDA CuBLAS operations\nwhen `torch.use_deterministic_algorithms(True)` is enabled.\n\nSee:\nhttps://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility\n\nIf this variable is set after importing torch, it will have no effect.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nos.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import Packages\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import functools\nfrom pathlib import Path\nimport mne\nimport torch\nimport braindecode\nimport rosoku\n\nfrom moabb.datasets import Dreyer2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define callback functions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def callback_get_model(X, y):\n    _, n_chans, n_times = X.shape\n    F1 = 4\n    D = 2\n    F2 = F1 * D\n\n    model = braindecode.models.EEGNet(\n        n_chans=n_chans,\n        n_outputs=2,\n        n_times=n_times,\n        F1=F1,\n        D=D,\n        F2=F2,\n        drop_prob=0.5,\n    )\n\n    return model\n\n\ndef callback_load_epochs(\n        items, split, dataset, l_freq, h_freq, order_filter, tmin, tmax\n):\n    subject = items[0]\n    items = items[1:]\n\n    sessions = dataset.get_data(subjects=[subject])\n    raws_dict = sessions[subject][\"0\"]\n\n    epochs_list = []\n\n    for name_run, raw in raws_dict.items():\n        if not True in [item in name_run for item in items]:\n            continue\n\n        raw.filter(\n            l_freq=l_freq,\n            h_freq=h_freq,\n            method=\"iir\",\n            iir_params={\"ftype\": \"butter\", \"order\": order_filter, \"btype\": \"bandpass\"},\n        )\n\n        raw = raw.pick(picks=\"eeg\")\n\n        epochs = mne.Epochs(\n            raw=raw,\n            tmin=tmin,\n            tmax=tmax,\n            baseline=None,\n        )\n\n        epochs_list.append(epochs)\n\n    return mne.concatenate_epochs(epochs_list)\n\n\ndef callback_proc_epochs(epochs, split):\n    # do nothing in this example\n    return epochs\n\n\ndef convert_epochs_to_ndarray(\n        epochs,\n        split,\n        label_keys,\n):\n    X = epochs.get_data()\n    y = rosoku.utils.get_labels_from_epochs(epochs, label_keys)\n\n    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run the Experiment using Early Stopping with Validation data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subject = 10\nresample = 128\n\nlr = 5e-4\nweight_decay = 1e-2\nn_epochs = 500\nbatch_size = 8\npatience = 75\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nseed = 42\n\ndataset = Dreyer2023()\n\nsave_base = Path(\"~\").expanduser() / \"rosoku-log\"\n(save_base / \"checkpoint\").mkdir(parents=True, exist_ok=True)\n(save_base / \"history\").mkdir(parents=True, exist_ok=True)\n(save_base / \"saliency\").mkdir(parents=True, exist_ok=True)\n(save_base / \"samples\").mkdir(parents=True, exist_ok=True)\n(save_base / \"normalization\").mkdir(parents=True, exist_ok=True)\n\ncriterion = torch.nn.CrossEntropyLoss()\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR\nscheduler_params = {\"T_max\": n_epochs, \"eta_min\": 1e-6}\noptimizer = torch.optim.AdamW\noptimizer_params = {\"lr\": lr, \"weight_decay\": weight_decay}\nearly_stopping = rosoku.utils.EarlyStopping(patience=patience)\n\nlabel_keys = {\"left_hand\": 0, \"right_hand\": 1}\n\nresults_1st_step = rosoku.deeplearning(\n    items_train=[subject, \"R1\", \"R2\", \"R3\"],\n    items_valid=[subject, \"R4\"],\n    items_test=[[subject, \"R5\", \"R6\"]],\n    callback_load_epochs=functools.partial(\n        callback_load_epochs,\n        dataset=dataset,\n        l_freq=8.0,\n        h_freq=30.0,\n        order_filter=4,\n        tmin=dataset.interval[0] + 0.5,\n        tmax=dataset.interval[1],\n    ),\n    callback_proc_epochs=callback_proc_epochs,\n    callback_convert_epochs_to_ndarray=functools.partial(\n        convert_epochs_to_ndarray, label_keys=label_keys\n    ),\n    batch_size=batch_size,\n    n_epochs=n_epochs,\n    criterion=criterion,\n    optimizer=optimizer,\n    optimizer_params=optimizer_params,\n    callback_get_model=callback_get_model,\n    scheduler=scheduler,\n    scheduler_params=scheduler_params,\n    device=device,\n    early_stopping=early_stopping,\n    history_fname=(save_base / \"history\" / f\"sub-{subject}.parquet\"),\n    checkpoint_fname=(save_base / \"checkpoint\" / f\"sub-{subject}.pth\"),\n    samples_fname=(save_base / \"samples\" / f\"sub-{subject}.parquet\"),\n    normalization_fname=(save_base / \"normalization\" / f\"sub-{subject}.msgpack\"),\n    saliency_map_fname=(save_base / \"saliency\" / f\"sub-{subject}.msgpack\"),\n    label_keys=label_keys,\n    seed=seed,\n    additional_values={\"subject\": subject},\n    use_deterministic_algorithms=True,\n    min_delta=0,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run the Experiment using Early Stopping with callback_early_stopping\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = torch.load(save_base / \"checkpoint\" / f\"sub-{subject}.pth\")\nloss_best = data[\"loss_best\"]\n\nlr = 5e-4\nweight_decay = 1e-2\nn_epochs = 500\nbatch_size = 8\nseed = 42\n\ncriterion = torch.nn.CrossEntropyLoss()\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR\nscheduler_params = {\"T_max\": n_epochs, \"eta_min\": 1e-6}\noptimizer = torch.optim.AdamW\noptimizer_params = {\"lr\": lr, \"weight_decay\": weight_decay}\nearly_stopping = rosoku.utils.EarlyStopping(patience=patience)\n\n\ndef callback_early_stopping(state):\n    # Stop training once the training loss reaches the best\n    # validation loss obtained in the first step\n    return state[\"train_loss\"] <= loss_best\n\n\nresults_2nd_step = rosoku.deeplearning(\n    items_train=[subject, \"R1\", \"R2\", \"R3\", \"R4\"],\n    items_valid=None,\n    items_test=[[subject, \"R5\", \"R6\"]],\n    callback_load_epochs=functools.partial(\n        callback_load_epochs,\n        dataset=dataset,\n        l_freq=8.0,\n        h_freq=30.0,\n        order_filter=4,\n        tmin=dataset.interval[0] + 0.5,\n        tmax=dataset.interval[1],\n    ),\n    callback_proc_epochs=callback_proc_epochs,\n    callback_convert_epochs_to_ndarray=functools.partial(\n        convert_epochs_to_ndarray, label_keys=label_keys\n    ),\n    callback_early_stopping=callback_early_stopping,\n    batch_size=batch_size,\n    n_epochs=n_epochs,\n    criterion=criterion,\n    optimizer=optimizer,\n    optimizer_params=optimizer_params,\n    callback_get_model=callback_get_model,\n    scheduler=scheduler,\n    scheduler_params=scheduler_params,\n    device=device,\n    history_fname=(save_base / \"history\" / f\"sub-{subject}_2nd.parquet\"),\n    checkpoint_fname=(save_base / \"checkpoint\" / f\"sub-{subject}_2nd.pth\"),\n    samples_fname=(save_base / \"samples\" / f\"sub-{subject}_2nd.parquet\"),\n    normalization_fname=(save_base / \"normalization\" / f\"sub-{subject}_2nd.msgpack\"),\n    saliency_map_fname=(save_base / \"saliency\" / f\"sub-{subject}_2nd.msgpack\"),\n    label_keys=label_keys,\n    seed=seed,\n    additional_values={\"subject\": subject},\n    use_deterministic_algorithms=True,\n    min_delta=0,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Print Results\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(results_1st_step.to_string())\nprint(results_2nd_step.to_string())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}